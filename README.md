# Exploratory Data Analysis (EDA) with Python


### Outline

- Project Overview
- Dataset Used
- Key Findings
- Libraries Used
- Further Directions


## Project Overview
This project demonstrates Exploratory Data Analysis (EDA) techniques using Python. The main focus is to uncover patterns, trends, and insights in the dataset. The key elements of the EDA include:

- Data visualization (covering univariate analysis, bivariate analysis, etc.)
- Feature engineering
- Identifying patterns and trends
- Investigating feature importance
- Checking assumptions

Focus areas for EDA:

- Explore Relationships with `Total Cup Points`
- Examine Linear/Non-Linear Patterns


  
You can view the full process in the Jupyter Notebook in this repo [here](https://github.com/Lillian1070/showcase_python_EDA_1/blob/main/kaggle_coffeeBean_EDA.ipynb). 


## Dataset Used
The dataset used in this project was sourced from [Kaggle](https://www.kaggle.com/datasets/fatihb/coffee-quality-data-cqi) and has been cleaned using Python in a separate [data cleaning project](https://github.com/Lillian1070/showcase_python_dataCleaning_1). The cleaned dataset includes 206 coffee samples from the Coffee Quality Institute (CQI), and it is characterized by features such as country of origin, company, altitude, harvest year, processing method, sensory attributes, moisture percentage, defects, color, expiration, certification body, and more. You can access the cleaned dataset [here](https://github.com/Lillian1070/showcase_python_dataCleaning_1/blob/main/cleaned_dataset.csv).


## Key Findings
Throughout the exploratory analysis, the following key insights were uncovered:

- **Relationships with `Total Cup Points`**: [Brief description of insight and its significance]
- **Linear & Non-Linear Patterns**: [Brief description of insight and its significance]


These findings provide a clearer picture of the dataset and its relationships, paving the way for more advanced analysis or predictive modeling.


## Libraries Used
- **Pandas**: For handling and manipulating the dataset, such as summarizing data, filtering, and grouping
- **Numpy**: For numerical operations like handling arrays and computing statistical measures
- **Matplotlib**: For data visualization, including plotting graphs like histograms, scatter plots, etc.
- **Seaborn**: For statistical data visualization and enhanced graphical representations. Used for heatmaps, pair plots, and correlation matrices to reveal deeper patterns in the data



# Further Directions
- xxx


ðŸ’¬ _Iâ€™d love to hear your thoughts! If you have any suggestions or questions, feel free to connect with me._


